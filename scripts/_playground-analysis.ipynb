{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f89ca7fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.colors as mc\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "import numpy as np\n",
    "import yaml\n",
    "import pypsa\n",
    "import calendar\n",
    "\n",
    "from matplotlib.cm import ScalarMappable\n",
    "from pypsa.descriptors import Dict\n",
    "from scipy import stats\n",
    "import seaborn as sns\n",
    "\n",
    "from math import radians, cos, sin, asin, sqrt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select run & Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c86f9b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_configuration(config_path):\n",
    "    \"\"\"\n",
    "    Load configuration settings from a YAML file.\n",
    "    \"\"\"\n",
    "    with open(config_path, \"r\") as f:\n",
    "        config = yaml.safe_load(f)\n",
    "    return config\n",
    "\n",
    "\n",
    "snakemake = Dict()\n",
    "snakemake.config = load_configuration(\"../config.yaml\")\n",
    "snakemake.input = Dict()\n",
    "snakemake.output = Dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9647a767",
   "metadata": {},
   "outputs": [],
   "source": [
    "run = \"test-newplots-1H-cfe100-spatial-noexcess\"  # run name from config.yaml\n",
    "distance = \"IEDK\"  # pair name from config.yaml\n",
    "\n",
    "if True:\n",
    "    folder = f\"/results/{run}\"\n",
    "    scenario = f\"/2025/p1/cfe100/{distance}\"\n",
    "\n",
    "    snakemake.input.data = f\"{folder}/networks/{scenario}/40.nc\"\n",
    "    snakemake.output.plot = f\"{folder}/plots/plot.pdf\"\n",
    "\n",
    "    n = pypsa.Network(f\"../{folder}/networks/{scenario}/40.nc\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "locations = list(snakemake.config[\"ci\"][f\"{distance}\"][\"datacenters\"].values())\n",
    "location0 = locations[0]\n",
    "location1 = locations[1]\n",
    "locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c913f79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# display two dataframes containing the time series\n",
    "df = n.generators_t.p_max_pu.filter(regex=f\"{location0}|{location1}\")\n",
    "df[\"2013-03-01\":\"2013-03-08\"].filter(like=\"solar\").plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = (\n",
    "    n.generators_t.p_max_pu.filter(regex=f\"{location0}|{location1}\")\n",
    "    .reset_index()\n",
    "    .rename(columns={\"index\": \"snapshot\"})\n",
    "    .assign(snapshot=lambda x: pd.to_datetime(x[\"snapshot\"]))\n",
    ")\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb64f70a",
   "metadata": {},
   "source": [
    "### Visualise time-series of RES feed-in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_heatmap_data(df, month, location, carrier, scaling, diff_location=None):\n",
    "    data = df[df[\"snapshot\"].dt.month == month]\n",
    "    day = data[\"snapshot\"].dt.day\n",
    "    value = data[f\"{location} {carrier}\"].values\n",
    "\n",
    "    if diff_location:\n",
    "        diff_value = data[f\"{diff_location} {carrier}\"].values\n",
    "        value -= diff_value  # Subtracting the second time-series from the first\n",
    "\n",
    "    value = value.reshape(int(24 / scaling), len(day.unique()), order=\"F\")\n",
    "    return day, value\n",
    "\n",
    "\n",
    "def draw_heatmap(ax, day, value, scaling, colormap, min_val, max_val):\n",
    "    xgrid = np.arange(day.max() + 1) + 1  # for days\n",
    "    ygrid = np.arange(int(24 / scaling) + 1)  # for hours\n",
    "\n",
    "    # Ensure the dimensions of 'value' match the expected dimensions for 'xgrid' and 'ygrid'\n",
    "    if value.shape != (len(ygrid) - 1, len(xgrid) - 1):\n",
    "        raise ValueError(\n",
    "            f\"Shape of value ({value.shape}) does not match xgrid ({len(xgrid)}) and ygrid ({len(ygrid)}) dimensions.\"\n",
    "        )\n",
    "\n",
    "    ax.pcolormesh(xgrid, ygrid, value, cmap=colormap, vmin=min_val, vmax=max_val)\n",
    "    ax.set_ylim(int(24 / scaling), 0)\n",
    "    ax.axis(\"off\")\n",
    "\n",
    "\n",
    "def plot_heatmap_cf(\n",
    "    df,\n",
    "    location,\n",
    "    carrier,\n",
    "    scaling,\n",
    "    colormap,\n",
    "    min_val,\n",
    "    max_val,\n",
    "    year=2013,\n",
    "    figsize=(14, 5),\n",
    "    plot_difference=False,\n",
    "    diff_location=None,\n",
    "):\n",
    "    fig, axes = plt.subplots(1, 12, figsize=figsize, sharey=True)\n",
    "    plt.tight_layout()\n",
    "\n",
    "    for month, ax in enumerate(axes, start=1):\n",
    "        # Pass additional parameters if plotting the difference\n",
    "        day, value = prepare_heatmap_data(\n",
    "            df,\n",
    "            month,\n",
    "            location,\n",
    "            carrier,\n",
    "            scaling,\n",
    "            diff_location=diff_location if plot_difference else None,\n",
    "        )\n",
    "        draw_heatmap(ax, day, value, scaling, colormap, min_val, max_val)\n",
    "        ax.set_title(calendar.month_abbr[month], fontsize=10, pad=3)\n",
    "\n",
    "    fig.subplots_adjust(\n",
    "        left=0.05, right=0.98, top=0.9, hspace=0.08, wspace=0.1, bottom=0.15\n",
    "    )\n",
    "    cbar_ax = fig.add_axes([0.3, 0.08, 0.4, 0.04])\n",
    "    norm = mc.Normalize(min_val, max_val)\n",
    "    cb = fig.colorbar(\n",
    "        ScalarMappable(norm=norm, cmap=colormap), cax=cbar_ax, orientation=\"horizontal\"\n",
    "    )\n",
    "    cb.set_label(\"Hourly Capacity Factor (%)\", size=14)\n",
    "\n",
    "    fig.text(0.12, 0.12, \"Day of the Month\", ha=\"center\", va=\"center\", fontsize=14)\n",
    "    fig.text(\n",
    "        0.04,\n",
    "        0.34,\n",
    "        \"Hour of the Day\",\n",
    "        ha=\"center\",\n",
    "        va=\"center\",\n",
    "        rotation=\"vertical\",\n",
    "        fontsize=14,\n",
    "    )\n",
    "\n",
    "    annotations = [\n",
    "        f\"Location: {location}\"\n",
    "        if not plot_difference\n",
    "        else f\"Location: {location} vs {diff_location}\",\n",
    "        f\"Carrier: {carrier}\",\n",
    "        f\"Weather Year: {year}\",\n",
    "        r\"Unit: MWhÂ·h$^{-1}$\",\n",
    "    ]\n",
    "    for i, annotation in enumerate(annotations):\n",
    "        fig.text(\n",
    "            0.95,\n",
    "            0.12 - i * 0.05,\n",
    "            annotation,\n",
    "            ha=\"right\",\n",
    "            va=\"center\",\n",
    "            fontsize=14,\n",
    "            color=\"black\",\n",
    "        )\n",
    "\n",
    "    fig.savefig(\"test.pdf\", bbox_inches=\"tight\", transparent=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "location = f\"{location1}\"\n",
    "carrier = \"solar\"\n",
    "scaling = int(snakemake.config[\"time_sampling\"][0])  # temporal scaling -- 3/1 for 3H/1H\n",
    "colormap = \"cividis\"  # \"cividis\" # \"RdBu\"  # https://matplotlib.org/stable/tutorials/colors/colormaps.html\n",
    "MIN, MAX = 0, 1  #  df[\"denmark onwind\"].min()\n",
    "\n",
    "plot_heatmap_cf(\n",
    "    df,\n",
    "    location,\n",
    "    carrier,\n",
    "    scaling,\n",
    "    colormap,\n",
    "    MIN,\n",
    "    MAX,\n",
    "    diff_location=f\"{location0}\",\n",
    "    plot_difference=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "location = f\"{location0}\"\n",
    "carrier = \"onwind\"\n",
    "scaling = int(snakemake.config[\"time_sampling\"][0])  # temporal scaling -- 3/1 for 3H/1H\n",
    "colormap = \"RdBu\"  # \"cividis\" # \"RdBu\"  # https://matplotlib.org/stable/tutorials/colors/colormaps.html\n",
    "MIN, MAX = -1, 1  #  df[\"denmark onwind\"].min()\n",
    "\n",
    "plot_heatmap_cf(\n",
    "    df,\n",
    "    location,\n",
    "    carrier,\n",
    "    scaling,\n",
    "    colormap,\n",
    "    MIN,\n",
    "    MAX,\n",
    "    diff_location=f\"{location1}\",\n",
    "    plot_difference=True,\n",
    ")\n",
    "\n",
    "# safe as pdf to local folder\n",
    "plt.savefig(\"onwind.pdf\", bbox_inches=\"tight\", transparent=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "location = f\"{location0}\"\n",
    "carrier = \"solar\"\n",
    "scaling = int(snakemake.config[\"time_sampling\"][0])  # temporal scaling -- 3/1 for 3H/1H\n",
    "colormap = \"RdBu\"  # \"cividis\" # \"RdBu\"  # https://matplotlib.org/stable/tutorials/colors/colormaps.html\n",
    "MIN, MAX = -1, 1  #  df[\"denmark onwind\"].min()\n",
    "\n",
    "plot_heatmap_cf(\n",
    "    df,\n",
    "    location,\n",
    "    carrier,\n",
    "    scaling,\n",
    "    colormap,\n",
    "    MIN,\n",
    "    MAX,\n",
    "    diff_location=f\"{location1}\",\n",
    "    plot_difference=True,\n",
    ")\n",
    "\n",
    "# safe as pdf to local folder\n",
    "plt.savefig(\"solar.pdf\", bbox_inches=\"tight\", transparent=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4df383d2",
   "metadata": {},
   "source": [
    "### Retrieve space-time shifts from model solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e722d00a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_nb(n, node, rename={}):\n",
    "    \"\"\"\n",
    "    Retrieve nodal energy balance per hour\n",
    "        -> lines and links are bidirectional AND their subsets are exclusive.\n",
    "        -> links include fossil gens\n",
    "    NB {-1} multiplier is a nodal balance sign\n",
    "    \"\"\"\n",
    "\n",
    "    components = [\"Generator\", \"Load\", \"StorageUnit\", \"Store\", \"Link\", \"Line\"]\n",
    "    nodal_balance = pd.DataFrame(index=n.snapshots)\n",
    "\n",
    "    for i in components:\n",
    "        if i == \"Generator\":\n",
    "            node_generators = n.generators.query(\"bus==@node\").index\n",
    "            nodal_balance = nodal_balance.join(n.generators_t.p[node_generators])\n",
    "        if i == \"Load\":\n",
    "            node_loads = n.loads.query(\"bus==@node\").index\n",
    "            nodal_balance = nodal_balance.join(-1 * n.loads_t.p_set[node_loads])\n",
    "        if i == \"Link\":\n",
    "            node_export_links = n.links.query(\"bus0==@node\").index\n",
    "            node_import_links = n.links.query(\"bus1==@node\").index\n",
    "            nodal_balance = nodal_balance.join(-1 * n.links_t.p0[node_export_links])\n",
    "            nodal_balance = nodal_balance.join(-1 * n.links_t.p1[node_import_links])\n",
    "            ##################\n",
    "        if i == \"StorageUnit\":\n",
    "            # node_storage_units = n.storage_units.query('bus==@node').index\n",
    "            # nodal_balance = nodal_balance.join(n.storage_units_t.p_dispatch[node_storage_units])\n",
    "            # nodal_balance = nodal_balance.join(n.storage_units_t.p_store[node_storage_units])\n",
    "            continue\n",
    "        if i == \"Line\":\n",
    "            continue\n",
    "        if i == \"Store\":\n",
    "            continue\n",
    "\n",
    "    nodal_balance = nodal_balance.rename(columns=rename).groupby(level=0, axis=1).sum()\n",
    "\n",
    "    # Custom groupby function\n",
    "    def custom_groupby(column_name):\n",
    "        if column_name.startswith(\"vcc\"):\n",
    "            return \"spatial shift\"\n",
    "        return column_name\n",
    "\n",
    "    # Apply custom groupby function\n",
    "    nodal_balance = nodal_balance.groupby(custom_groupby, axis=1).sum()\n",
    "\n",
    "    # revert nodal balance sign for display\n",
    "    if \"spatial shift\" in nodal_balance.columns:\n",
    "        nodal_balance[\"spatial shift\"] = nodal_balance[\"spatial shift\"] * -1\n",
    "    if \"temporal shift\" in nodal_balance.columns:\n",
    "        nodal_balance[\"temporal shift\"] = nodal_balance[\"temporal shift\"] * -1\n",
    "\n",
    "    return nodal_balance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hourly_curtailment(network, tech, buses):\n",
    "    \"\"\"\n",
    "    Calculate the curtailment for a given technology and bus.\n",
    "\n",
    "    :param network: The PyPSA network object\n",
    "    :param tech: Technology type (string)\n",
    "    :param buses: Buses to consider for the calculation (iterable of strings)\n",
    "    :param weights: Weights for the calculation (pandas Series or similar)\n",
    "    :return: Total curtailment for the given technology and buses\n",
    "    \"\"\"\n",
    "    weights = n.snapshot_weightings[\"generators\"]\n",
    "    gens = network.generators.query(\"carrier == @tech and bus in @buses\").index\n",
    "    curtailment = (\n",
    "        (\n",
    "            network.generators_t.p_max_pu[gens] * network.generators.p_nom_opt[gens]\n",
    "            - network.generators_t.p[gens]\n",
    "        )\n",
    "        .clip(lower=0)\n",
    "        .multiply(weights, axis=0)\n",
    "        .sum(axis=1)\n",
    "    )\n",
    "    return curtailment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_datacenter_shifts(n, dc1, dc2):\n",
    "    \"\"\"\n",
    "    Analyze the shifts in energy feed-in and spatial shift for two datacenters.\n",
    "\n",
    "    :param n: PyPSA Network object\n",
    "    :param dc1: Name of the first datacenter\n",
    "    :param dc2: Name of the second datacenter\n",
    "    :return: Dictionary with analysis results\n",
    "\n",
    "    NB Positive shift -> sending jobs away; negative shift -> receiving jobs\n",
    "    \"\"\"\n",
    "\n",
    "    # retrieve datacenter data\n",
    "    def analyze_dc(dc_name):\n",
    "        feedin = retrieve_nb(n, dc_name)[[f\"{dc_name} onwind\", f\"{dc_name} solar\"]]\n",
    "        curtailment = hourly_curtailment(n, \"onwind\", [dc_name]) + hourly_curtailment(\n",
    "            n, \"solar\", [dc_name]\n",
    "        )\n",
    "        spatial_shift = retrieve_nb(n, dc_name)[\"spatial shift\"]\n",
    "\n",
    "        return {\n",
    "            \"feedin\": feedin,\n",
    "            \"curtailment\": curtailment,\n",
    "            \"spatial_shift\": spatial_shift,\n",
    "        }\n",
    "\n",
    "    # Analyze both datacenters\n",
    "    dc1_analysis = analyze_dc(dc1)\n",
    "    dc2_analysis = analyze_dc(dc2)\n",
    "\n",
    "    # Collect and store wind and solar hourly potentials\n",
    "    potentials_dc1 = n.generators_t.p_max_pu[[f\"{dc1} onwind\", f\"{dc1} solar\"]]\n",
    "    potentials_dc2 = n.generators_t.p_max_pu[[f\"{dc2} onwind\", f\"{dc2} solar\"]]\n",
    "\n",
    "    # Compute differences between wind and solar feed-in\n",
    "    diff_onwind = (\n",
    "        dc1_analysis[\"feedin\"][f\"{dc1} onwind\"]\n",
    "        - dc2_analysis[\"feedin\"][f\"{dc2} onwind\"]\n",
    "    )\n",
    "    diff_solar = (\n",
    "        dc1_analysis[\"feedin\"][f\"{dc1} solar\"] - dc2_analysis[\"feedin\"][f\"{dc2} solar\"]\n",
    "    )\n",
    "\n",
    "    # Compute differences between wind and solar potentials\n",
    "    diff_onwind_potential = (\n",
    "        potentials_dc1[f\"{dc1} onwind\"] - potentials_dc2[f\"{dc2} onwind\"]\n",
    "    )\n",
    "    diff_solar_potential = (\n",
    "        potentials_dc1[f\"{dc1} solar\"] - potentials_dc2[f\"{dc2} solar\"]\n",
    "    )\n",
    "\n",
    "    return {\n",
    "        f\"{dc1}\": {\n",
    "            \"feedin\": dc1_analysis[\"feedin\"],\n",
    "            \"potentials\": potentials_dc1,\n",
    "            \"curtailment\": dc1_analysis[\"curtailment\"],\n",
    "            \"spatial_shift\": dc1_analysis[\"spatial_shift\"],\n",
    "        },\n",
    "        f\"{dc2}\": {\n",
    "            \"feedin\": dc2_analysis[\"feedin\"],\n",
    "            \"potentials\": potentials_dc2,\n",
    "            \"curtailment\": dc2_analysis[\"curtailment\"],\n",
    "            \"spatial_shift\": dc2_analysis[\"spatial_shift\"],\n",
    "        },\n",
    "        \"diff_generation\": {\"onwind\": diff_onwind, \"solar\": diff_solar},\n",
    "        \"diff_potentials\": {\n",
    "            \"onwind\": diff_onwind_potential,\n",
    "            \"solar\": diff_solar_potential,\n",
    "        },\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NB Positive shift -> sending jobs away; negative shift -> receiving jobs\n",
    "\n",
    "correlation_coefficient, p_value = stats.pearsonr(\n",
    "    analyze_datacenter_shifts(n, dc1=f\"{location0}\", dc2=f\"{location1}\")[\n",
    "        f\"{location0}\"\n",
    "    ][\"spatial_shift\"],\n",
    "    -(\n",
    "        analyze_datacenter_shifts(n, dc1=f\"{location0}\", dc2=f\"{location1}\")[\n",
    "            \"diff_generation\"\n",
    "        ][\"onwind\"]\n",
    "        + analyze_datacenter_shifts(n, dc1=f\"{location0}\", dc2=f\"{location1}\")[\n",
    "            \"diff_generation\"\n",
    "        ][\"solar\"]\n",
    "    ),\n",
    ")\n",
    "# -analyze_datacenter_shifts(n, dc1=f\"{location0}\", dc2=f\"{location1}\")['Ireland'][\"curtailment\"]\n",
    "\n",
    "print(location0, location1)\n",
    "print(f\"Correlation Coefficient: {round(correlation_coefficient, 3)}\")\n",
    "print(f\"p-value: {round(p_value,3)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "711537a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Potentials correlation\n",
    "corr_matrix = np.corrcoef(df[f\"{location0} solar\"], df[f\"{location1} solar\"])\n",
    "corr_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Potentials correlation\n",
    "corr_matrix = np.corrcoef(df[f\"{location0} onwind\"], df[f\"{location1} onwind\"])\n",
    "corr_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generation correlation\n",
    "corr_matrix = np.corrcoef(\n",
    "    analyze_datacenter_shifts(n, dc1=f\"{location0}\", dc2=f\"{location1}\")[\n",
    "        f\"{location0}\"\n",
    "    ][\"feedin\"][f\"{location0} solar\"],\n",
    "    analyze_datacenter_shifts(n, dc1=f\"{location1}\", dc2=f\"{location0}\")[\n",
    "        f\"{location1}\"\n",
    "    ][\"feedin\"][f\"{location1} solar\"],\n",
    ")\n",
    "corr_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generation correlation\n",
    "corr_matrix = np.corrcoef(\n",
    "    analyze_datacenter_shifts(n, dc1=f\"{location0}\", dc2=f\"{location1}\")[\n",
    "        f\"{location0}\"\n",
    "    ][\"feedin\"][f\"{location0} onwind\"],\n",
    "    analyze_datacenter_shifts(n, dc1=f\"{location1}\", dc2=f\"{location0}\")[\n",
    "        f\"{location1}\"\n",
    "    ][\"feedin\"][f\"{location1} onwind\"],\n",
    ")\n",
    "corr_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cost savings VS distance/corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_distance(n, bus1, bus2):\n",
    "    \"\"\"\n",
    "    Calculate the great circle distance between two buses in a PyPSA network object using the Haversine formula.\n",
    "\n",
    "    Parameters:\n",
    "    n (DataFrame): PyPSA network object containing bus coordinates.\n",
    "    bus1 (str): The ID of the first bus.\n",
    "    bus2 (str): The ID of the second bus.\n",
    "\n",
    "    Returns:\n",
    "    float: The distance between the two buses in kilometers.\n",
    "    \"\"\"\n",
    "\n",
    "    def haversine(lon1, lat1, lon2, lat2):\n",
    "        \"\"\"\n",
    "        Calculate the great circle distance between two points on the earth (specified in decimal degrees)\n",
    "        \"\"\"\n",
    "        # Convert decimal degrees to radians\n",
    "        lon1, lat1, lon2, lat2 = map(radians, [lon1, lat1, lon2, lat2])\n",
    "\n",
    "        dlon = lon2 - lon1\n",
    "        dlat = lat2 - lat1\n",
    "        a = sin(dlat / 2) ** 2 + cos(lat1) * cos(lat2) * sin(dlon / 2) ** 2\n",
    "        c = 2 * asin(sqrt(a))\n",
    "        r = 6371  # Radius of earth in kilometers.\n",
    "        return c * r\n",
    "\n",
    "    # Extract the coordinates of the two buses\n",
    "    lon1, lat1 = n.buses.loc[bus1, [\"x\", \"y\"]]\n",
    "    lon2, lat2 = n.buses.loc[bus2, [\"x\", \"y\"]]\n",
    "\n",
    "    # Calculate the distance using the Haversine formula\n",
    "    distance_km = haversine(lon1, lat1, lon2, lat2)\n",
    "\n",
    "    return distance_km"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scenarios = snakemake.config[\"scenario\"][\"distance\"]\n",
    "flexibilities = snakemake.config[\"scenario\"][\"flexibility\"]\n",
    "flexibilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "\n",
    "for scenario in scenarios:\n",
    "    for flexibility in flexibilities:\n",
    "        n = pypsa.Network(\n",
    "            f\"../{folder}/networks/2025/p1/cfe100/{scenario}/{flexibility}.nc\"\n",
    "        )\n",
    "\n",
    "        file_path = f\"..{folder}/summaries/2025/p1/cfe100/{scenario}/{flexibility}.yaml\"\n",
    "        with open(file_path, \"r\") as f:\n",
    "            summary = yaml.safe_load(f)\n",
    "\n",
    "            # Check if there is only one location\n",
    "            if len(summary) == 1:\n",
    "                location = next(iter(summary))  # Get the single location key\n",
    "                values = summary[location]\n",
    "                ci_average_cost = values.get(\"ci_average_cost\", None)\n",
    "                ci_total_cost = round(values.get(\"ci_total_cost\", None) / 1e6, 1)\n",
    "                if ci_average_cost is not None:\n",
    "                    data.append(\n",
    "                        (\n",
    "                            scenario,\n",
    "                            flexibility,\n",
    "                            location,\n",
    "                            0,  # Distance is 0 for a single location\n",
    "                            ci_average_cost,\n",
    "                            ci_total_cost,\n",
    "                        )\n",
    "                    )\n",
    "            else:\n",
    "                # If more than one location, calculate distances as before\n",
    "                for location, values in summary.items():\n",
    "                    ci_average_cost = values.get(\"ci_average_cost\", None)\n",
    "                    ci_total_cost = round(values.get(\"ci_total_cost\", None) / 1e6, 1)\n",
    "                    if ci_average_cost is not None:\n",
    "                        for other_location in summary:\n",
    "                            if other_location != location:\n",
    "                                distance = round(\n",
    "                                    calculate_distance(n, location, other_location), 1\n",
    "                                )\n",
    "                                data.append(\n",
    "                                    (\n",
    "                                        scenario,\n",
    "                                        flexibility,\n",
    "                                        location,\n",
    "                                        distance,\n",
    "                                        ci_average_cost,\n",
    "                                        ci_total_cost,\n",
    "                                    )\n",
    "                                )\n",
    "\n",
    "# Create a multi-index DataFrame\n",
    "df = pd.DataFrame(\n",
    "    data,\n",
    "    columns=[\n",
    "        \"Scenario\",\n",
    "        \"Flexibility\",\n",
    "        \"Location\",\n",
    "        \"Distance\",\n",
    "        \"CI_Average_Cost\",\n",
    "        \"CI_Total_Cost\",\n",
    "    ],\n",
    ")\n",
    "df.set_index([\"Scenario\", \"Flexibility\", \"Location\"], inplace=True)\n",
    "\n",
    "# Display the DataFrame\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset the index of the dataframe\n",
    "df_reset = df.reset_index()\n",
    "\n",
    "# Sum Total Costs across locations for each Scenario and Flexibility\n",
    "total_costs_df = (\n",
    "    df_reset.groupby([\"Scenario\", \"Flexibility\"])\n",
    "    .agg(\n",
    "        {\n",
    "            \"CI_Total_Cost\": \"sum\",\n",
    "            \"Distance\": \"mean\",  # Assuming you want to also average the distance for each scenario and flexibility\n",
    "        }\n",
    "    )\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "# Calculate Baseline Costs (0% Flexibility) for each Scenario\n",
    "baseline_costs = total_costs_df[total_costs_df[\"Flexibility\"] == \"0\"][\n",
    "    [\"Scenario\", \"CI_Total_Cost\"]\n",
    "].rename(columns={\"CI_Total_Cost\": \"Baseline_Cost\"})\n",
    "\n",
    "# Merge the baseline costs with the total_costs_df\n",
    "df_with_baseline = total_costs_df.merge(baseline_costs, on=\"Scenario\")\n",
    "\n",
    "# Calculate Cost Savings for each Scenario and Flexibility compared to Baseline\n",
    "df_with_baseline[\"Cost_Savings\"] = round(\n",
    "    100\n",
    "    * (df_with_baseline[\"Baseline_Cost\"] - df_with_baseline[\"CI_Total_Cost\"])\n",
    "    / df_with_baseline[\"Baseline_Cost\"],\n",
    "    2,\n",
    ")\n",
    "\n",
    "# Now df_with_baseline contains Scenario, Flexibility, mean Distance, Total Cost, Baseline Cost, and Cost Savings\n",
    "df_with_baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(style=\"ticks\")\n",
    "sns.set_context(\"talk\", rc={\"lines.linewidth\": 3})\n",
    "\n",
    "\n",
    "def plot_cost_savings(df):\n",
    "    \"\"\"\n",
    "    Plot cost savings as a function of distance with different hues for each flexibility scenario using Seaborn.\n",
    "\n",
    "    Parameters:\n",
    "    df (pandas.DataFrame): DataFrame containing 'Distance', 'Cost_Savings', and 'Flexibility' columns.\n",
    "    \"\"\"\n",
    "    # Create the plot with higher DPI for better quality\n",
    "    plt.figure(figsize=(8, 6), dpi=120)\n",
    "\n",
    "    # Create a line plot with different hues for each flexibility scenario\n",
    "    # Use the same marker for each data point\n",
    "    lineplot = sns.lineplot(\n",
    "        x=\"Distance\",\n",
    "        y=\"Cost_Savings\",\n",
    "        hue=\"Flexibility\",\n",
    "        style=\"Flexibility\",\n",
    "        data=df,\n",
    "        markers=\"o\",\n",
    "        dashes=False,\n",
    "        palette=\"deep\",\n",
    "    )\n",
    "\n",
    "    # Set the plot title and labels\n",
    "    plt.title(\"Cost savings by datacenter distance \\n and share of flexible loads\")\n",
    "    plt.xlabel(\"Haversine distance between datacenter pair (km)\")\n",
    "    plt.ylabel(\"Cost Savings (%)\")\n",
    "\n",
    "    # Adjust the legend to show only line markers\n",
    "    handles, labels = lineplot.get_legend_handles_labels()\n",
    "    # add % to legend labels\n",
    "    labels = [f\"{label}%\" for label in labels]\n",
    "    plt.legend(\n",
    "        loc=\"upper left\", fontsize=\"small\", ncol=2, handles=handles[:], labels=labels[:]\n",
    "    )\n",
    "\n",
    "    # Display the plot\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Assuming df_with_baseline is the DataFrame prepared earlier\n",
    "plot_cost_savings(df_with_baseline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "",
   "language": "python",
   "name": ""
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
