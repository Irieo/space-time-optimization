{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f89ca7fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.colors as mc\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "import numpy as np\n",
    "import yaml\n",
    "import pypsa\n",
    "import calendar\n",
    "\n",
    "from matplotlib.cm import ScalarMappable\n",
    "from pypsa.descriptors import Dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select run & Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c86f9b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_configuration(config_path):\n",
    "    \"\"\"\n",
    "    Load configuration settings from a YAML file.\n",
    "    \"\"\"\n",
    "    with open(config_path, \"r\") as f:\n",
    "        config = yaml.safe_load(f)\n",
    "    return config\n",
    "\n",
    "\n",
    "snakemake = Dict()\n",
    "snakemake.config = load_configuration(\"../config.yaml\")\n",
    "snakemake.input = Dict()\n",
    "snakemake.output = Dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9647a767",
   "metadata": {},
   "outputs": [],
   "source": [
    "run = \"test-IEDK-1H-cfe100-allflex\"  # run name from config.yaml\n",
    "\n",
    "if True:\n",
    "    folder = f\"/results/{run}\"\n",
    "    scenario = \"/2025/IEDK/p1/cfe100\"\n",
    "\n",
    "    snakemake.input.data = f\"{folder}/networks/{scenario}/40.nc\"\n",
    "    snakemake.output.plot = f\"{folder}/plots/plot.pdf\"\n",
    "\n",
    "    n = pypsa.Network(f\"../{folder}/networks/{scenario}/40.nc\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df8e588a",
   "metadata": {},
   "outputs": [],
   "source": [
    "n.generators_t.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c913f79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# display two dataframes containing the time series\n",
    "df = n.generators_t.p_max_pu.filter(regex=\"Ireland|Denmark\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = (\n",
    "    n.generators_t.p_max_pu.filter(regex=\"Ireland|Denmark\")\n",
    "    .reset_index()\n",
    "    .rename(columns={\"index\": \"snapshot\"})\n",
    "    .assign(snapshot=lambda x: pd.to_datetime(x[\"snapshot\"]))\n",
    ")\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb64f70a",
   "metadata": {},
   "source": [
    "### Visualise time-series of RES feed-in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_heatmap_data(df, month, location, carrier, scaling, diff_location=None):\n",
    "    data = df[df[\"snapshot\"].dt.month == month]\n",
    "    day = data[\"snapshot\"].dt.day\n",
    "    value = data[f\"{location} {carrier}\"].values\n",
    "\n",
    "    if diff_location:\n",
    "        diff_value = data[f\"{diff_location} {carrier}\"].values\n",
    "        value -= diff_value  # Subtracting the second time-series from the first\n",
    "\n",
    "    value = value.reshape(int(24 / scaling), len(day.unique()), order=\"F\")\n",
    "    return day, value\n",
    "\n",
    "\n",
    "def draw_heatmap(ax, day, value, scaling, colormap, min_val, max_val):\n",
    "    xgrid = np.arange(day.max() + 1) + 1  # for days\n",
    "    ygrid = np.arange(int(24 / scaling) + 1)  # for hours\n",
    "\n",
    "    # Ensure the dimensions of 'value' match the expected dimensions for 'xgrid' and 'ygrid'\n",
    "    if value.shape != (len(ygrid) - 1, len(xgrid) - 1):\n",
    "        raise ValueError(\n",
    "            f\"Shape of value ({value.shape}) does not match xgrid ({len(xgrid)}) and ygrid ({len(ygrid)}) dimensions.\"\n",
    "        )\n",
    "\n",
    "    ax.pcolormesh(xgrid, ygrid, value, cmap=colormap, vmin=min_val, vmax=max_val)\n",
    "    ax.set_ylim(int(24 / scaling), 0)\n",
    "    ax.axis(\"off\")\n",
    "\n",
    "\n",
    "def plot_heatmap_cf(\n",
    "    df,\n",
    "    location,\n",
    "    carrier,\n",
    "    scaling,\n",
    "    colormap,\n",
    "    min_val,\n",
    "    max_val,\n",
    "    year=2013,\n",
    "    figsize=(14, 5),\n",
    "    plot_difference=False,\n",
    "    diff_location=None,\n",
    "):\n",
    "    fig, axes = plt.subplots(1, 12, figsize=figsize, sharey=True)\n",
    "    plt.tight_layout()\n",
    "\n",
    "    for month, ax in enumerate(axes, start=1):\n",
    "        # Pass additional parameters if plotting the difference\n",
    "        day, value = prepare_heatmap_data(\n",
    "            df,\n",
    "            month,\n",
    "            location,\n",
    "            carrier,\n",
    "            scaling,\n",
    "            diff_location=diff_location if plot_difference else None,\n",
    "        )\n",
    "        draw_heatmap(ax, day, value, scaling, colormap, min_val, max_val)\n",
    "        ax.set_title(calendar.month_abbr[month], fontsize=10, pad=3)\n",
    "\n",
    "    fig.subplots_adjust(\n",
    "        left=0.05, right=0.98, top=0.9, hspace=0.08, wspace=0.1, bottom=0.15\n",
    "    )\n",
    "    cbar_ax = fig.add_axes([0.3, 0.08, 0.4, 0.04])\n",
    "    norm = mc.Normalize(min_val, max_val)\n",
    "    cb = fig.colorbar(\n",
    "        ScalarMappable(norm=norm, cmap=colormap), cax=cbar_ax, orientation=\"horizontal\"\n",
    "    )\n",
    "    cb.set_label(\"Hourly Capacity Factor (%)\", size=14)\n",
    "\n",
    "    fig.text(0.12, 0.12, \"Day of the Month\", ha=\"center\", va=\"center\", fontsize=14)\n",
    "    fig.text(\n",
    "        0.04,\n",
    "        0.34,\n",
    "        \"Hour of the Day\",\n",
    "        ha=\"center\",\n",
    "        va=\"center\",\n",
    "        rotation=\"vertical\",\n",
    "        fontsize=14,\n",
    "    )\n",
    "\n",
    "    annotations = [\n",
    "        f\"Location: {location}\"\n",
    "        if not plot_difference\n",
    "        else f\"Location: {location} vs {diff_location}\",\n",
    "        f\"Carrier: {carrier}\",\n",
    "        f\"Weather Year: {year}\",\n",
    "        r\"Unit: MWhÂ·h$^{-1}$\",\n",
    "    ]\n",
    "    for i, annotation in enumerate(annotations):\n",
    "        fig.text(\n",
    "            0.95,\n",
    "            0.12 - i * 0.05,\n",
    "            annotation,\n",
    "            ha=\"right\",\n",
    "            va=\"center\",\n",
    "            fontsize=14,\n",
    "            color=\"black\",\n",
    "        )\n",
    "\n",
    "    fig.savefig(\"test.pdf\", bbox_inches=\"tight\", transparent=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "location = \"Ireland\"\n",
    "carrier = \"solar\"\n",
    "scaling = int(snakemake.config[\"time_sampling\"][0])  # temporal scaling -- 3/1 for 3H/1H\n",
    "colormap = \"RdBu\"  # \"cividis\" # \"RdBu\"  # https://matplotlib.org/stable/tutorials/colors/colormaps.html\n",
    "MIN, MAX = -1, 1  #  df[\"denmark onwind\"].min()\n",
    "\n",
    "plot_heatmap_cf(\n",
    "    df,\n",
    "    location,\n",
    "    carrier,\n",
    "    scaling,\n",
    "    colormap,\n",
    "    MIN,\n",
    "    MAX,\n",
    "    diff_location=\"Denmark\",\n",
    "    plot_difference=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4df383d2",
   "metadata": {},
   "source": [
    "### Retrieve space-time shifts from model solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e722d00a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_nb(n, node, rename={}):\n",
    "    \"\"\"\n",
    "    Retrieve nodal energy balance per hour\n",
    "        -> lines and links are bidirectional AND their subsets are exclusive.\n",
    "        -> links include fossil gens\n",
    "    NB {-1} multiplier is a nodal balance sign\n",
    "    \"\"\"\n",
    "\n",
    "    components = [\"Generator\", \"Load\", \"StorageUnit\", \"Store\", \"Link\", \"Line\"]\n",
    "    nodal_balance = pd.DataFrame(index=n.snapshots)\n",
    "\n",
    "    for i in components:\n",
    "        if i == \"Generator\":\n",
    "            node_generators = n.generators.query(\"bus==@node\").index\n",
    "            nodal_balance = nodal_balance.join(n.generators_t.p[node_generators])\n",
    "        if i == \"Load\":\n",
    "            node_loads = n.loads.query(\"bus==@node\").index\n",
    "            nodal_balance = nodal_balance.join(-1 * n.loads_t.p_set[node_loads])\n",
    "        if i == \"Link\":\n",
    "            node_export_links = n.links.query(\"bus0==@node\").index\n",
    "            node_import_links = n.links.query(\"bus1==@node\").index\n",
    "            nodal_balance = nodal_balance.join(-1 * n.links_t.p0[node_export_links])\n",
    "            nodal_balance = nodal_balance.join(-1 * n.links_t.p1[node_import_links])\n",
    "            ##################\n",
    "        if i == \"StorageUnit\":\n",
    "            # node_storage_units = n.storage_units.query('bus==@node').index\n",
    "            # nodal_balance = nodal_balance.join(n.storage_units_t.p_dispatch[node_storage_units])\n",
    "            # nodal_balance = nodal_balance.join(n.storage_units_t.p_store[node_storage_units])\n",
    "            continue\n",
    "        if i == \"Line\":\n",
    "            continue\n",
    "        if i == \"Store\":\n",
    "            continue\n",
    "\n",
    "    nodal_balance = nodal_balance.rename(columns=rename).groupby(level=0, axis=1).sum()\n",
    "\n",
    "    # Custom groupby function\n",
    "    def custom_groupby(column_name):\n",
    "        if column_name.startswith(\"vcc\"):\n",
    "            return \"spatial shift\"\n",
    "        return column_name\n",
    "\n",
    "    # Apply custom groupby function\n",
    "    nodal_balance = nodal_balance.groupby(custom_groupby, axis=1).sum()\n",
    "\n",
    "    # revert nodal balance sign for display\n",
    "    if \"spatial shift\" in nodal_balance.columns:\n",
    "        nodal_balance[\"spatial shift\"] = nodal_balance[\"spatial shift\"] * -1\n",
    "    if \"temporal shift\" in nodal_balance.columns:\n",
    "        nodal_balance[\"temporal shift\"] = nodal_balance[\"temporal shift\"] * -1\n",
    "\n",
    "    return nodal_balance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hourly_curtailment(network, tech, buses):\n",
    "    \"\"\"\n",
    "    Calculate the curtailment for a given technology and bus.\n",
    "\n",
    "    :param network: The PyPSA network object\n",
    "    :param tech: Technology type (string)\n",
    "    :param buses: Buses to consider for the calculation (iterable of strings)\n",
    "    :param weights: Weights for the calculation (pandas Series or similar)\n",
    "    :return: Total curtailment for the given technology and buses\n",
    "    \"\"\"\n",
    "    weights = n.snapshot_weightings[\"generators\"]\n",
    "    gens = network.generators.query(\"carrier == @tech and bus in @buses\").index\n",
    "    curtailment = (\n",
    "        (\n",
    "            network.generators_t.p_max_pu[gens] * network.generators.p_nom_opt[gens]\n",
    "            - network.generators_t.p[gens]\n",
    "        )\n",
    "        .clip(lower=0)\n",
    "        .multiply(weights, axis=0)\n",
    "        .sum(axis=1)\n",
    "    )\n",
    "    return curtailment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hourly_curtailment(n, \"solar\", [\"Ireland\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieve_nb(n, names[1]).columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dc1 = names[0]\n",
    "dc2 = names[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_datacenter_shifts(n, dc1, dc2):\n",
    "    \"\"\"\n",
    "    Analyze the shifts in energy feed-in and spatial shift for two datacenters.\n",
    "\n",
    "    :param n: PyPSA Network object\n",
    "    :param dc1: Name of the first datacenter\n",
    "    :param dc2: Name of the second datacenter\n",
    "    :return: Dictionary with analysis results\n",
    "\n",
    "    NB Positive shift -> sending jobs away; negative shift -> receiving jobs\n",
    "    \"\"\"\n",
    "\n",
    "    # retrieve datacenter data\n",
    "    def analyze_dc(dc_name):\n",
    "        feedin = retrieve_nb(n, dc_name)[[f\"{dc_name} onwind\", f\"{dc_name} solar\"]]\n",
    "        curtailment = hourly_curtailment(n, \"onwind\", [dc_name]) + hourly_curtailment(\n",
    "            n, \"solar\", [dc_name]\n",
    "        )\n",
    "        spatial_shift = retrieve_nb(n, dc_name)[\"spatial shift\"]\n",
    "\n",
    "        return {\n",
    "            \"feedin\": feedin,\n",
    "            \"curtailment\": curtailment,\n",
    "            \"spatial_shift\": spatial_shift,\n",
    "        }\n",
    "\n",
    "    # Analyze both datacenters\n",
    "    dc1_analysis = analyze_dc(dc1)\n",
    "    dc2_analysis = analyze_dc(dc2)\n",
    "\n",
    "    # Compute differences between wind and solar feed-in\n",
    "    diff_onwind = (\n",
    "        dc1_analysis[\"feedin\"][f\"{dc1} onwind\"]\n",
    "        - dc2_analysis[\"feedin\"][f\"{dc2} onwind\"]\n",
    "    )\n",
    "    diff_solar = (\n",
    "        dc1_analysis[\"feedin\"][f\"{dc1} solar\"] - dc2_analysis[\"feedin\"][f\"{dc2} solar\"]\n",
    "    )\n",
    "\n",
    "    # Collect and store wind and solar hourly potentials\n",
    "    potentials_dc1 = n.generators_t.p_max_pu[[f\"{dc1} onwind\", f\"{dc1} solar\"]]\n",
    "    potentials_dc2 = n.generators_t.p_max_pu[[f\"{dc2} onwind\", f\"{dc2} solar\"]]\n",
    "\n",
    "    return {\n",
    "        f\"{dc1}\": {\n",
    "            \"feedin\": dc1_analysis[\"feedin\"],\n",
    "            \"potentials\": potentials_dc1,\n",
    "            \"curtailment\": dc1_analysis[\"curtailment\"],\n",
    "            \"spatial_shift\": dc1_analysis[\"spatial_shift\"],\n",
    "        },\n",
    "        f\"{dc2}\": {\n",
    "            \"feedin\": dc2_analysis[\"feedin\"],\n",
    "            \"potentials\": potentials_dc2,\n",
    "            \"curtailment\": dc2_analysis[\"curtailment\"],\n",
    "            \"spatial_shift\": dc2_analysis[\"spatial_shift\"],\n",
    "        },\n",
    "        \"diff_series\": {\"onwind\": diff_onwind, \"solar\": diff_solar},\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyze_datacenter_shifts(n, dc1=\"Denmark\", dc2=\"Ireland\")[\"diff_series\"][\n",
    "    \"solar\"\n",
    "].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e9b840e",
   "metadata": {},
   "outputs": [],
   "source": [
    "(df[f\"Denmark {carrier}\"] - df[f\"Ireland {carrier}\"])[:720].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f50924c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Positive value -> Denmark is better resources; negative values -> Ireland has better resources\n",
    "value = df[f\"Denmark {carrier}\"] - df[f\"Ireland {carrier}\"]\n",
    "value.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d4645e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate Pearson correlation coefficient\n",
    "corr_matrix = np.corrcoef(spatial_shift, -value)\n",
    "corr_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bed27ea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "\n",
    "# Assuming value and spatial_shift are your data arrays\n",
    "correlation_coefficient, p_value = stats.pearsonr(spatial_shift, -value)\n",
    "\n",
    "print(f\"Correlation Coefficient: {correlation_coefficient}\")\n",
    "print(f\"p-value: {p_value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "711537a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_matrix = np.corrcoef(df[f\"Denmark {carrier}\"], df[f\"Ireland {carrier}\"])\n",
    "corr_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "",
   "language": "python",
   "name": ""
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
