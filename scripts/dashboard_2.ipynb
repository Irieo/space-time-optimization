{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib as mpl\n",
    "\n",
    "matplotlib.use(\"PDF\")\n",
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "from matplotlib.cm import ScalarMappable\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.colors as mc\n",
    "import matplotlib.pyplot as plt\n",
    "import yaml\n",
    "import pypsa\n",
    "import calendar\n",
    "from pypsa.descriptors import Dict\n",
    "import seaborn as sns\n",
    "\n",
    "from cartopy import crs as ccrs\n",
    "import cartopy.feature as cfeature\n",
    "import geopandas as gpd\n",
    "\n",
    "from math import radians, cos, sin, asin, sqrt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_configuration(config_path):\n",
    "    \"\"\"\n",
    "    Load configuration settings from a YAML file.\n",
    "    \"\"\"\n",
    "    with open(config_path, \"r\") as f:\n",
    "        config = yaml.safe_load(f)\n",
    "    return config\n",
    "\n",
    "\n",
    "snakemake = Dict()\n",
    "snakemake.config = load_configuration(\"../config.yaml\")\n",
    "snakemake.input = Dict()\n",
    "snakemake.output = Dict()\n",
    "\n",
    "run = \"test-distances2-1H-allflex-noexcess-nocostshifts\"  # run name from config.yaml\n",
    "distance = \"IEDK\"  # pair name from config.yaml\n",
    "\n",
    "if True:\n",
    "    folder = f\"/results/{run}\"\n",
    "    scenario = f\"/2025/p1/cfe100/{distance}\"\n",
    "\n",
    "    snakemake.input.data = f\"{folder}/networks/{scenario}/40.nc\"\n",
    "    snakemake.output.plot = f\"{folder}/plots/plot.pdf\"\n",
    "\n",
    "    n = pypsa.Network(f\"../{folder}/networks/{scenario}/40.nc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = snakemake.config\n",
    "policy = \"cfe100\"\n",
    "palette = \"p1\"\n",
    "zone = snakemake.config[\"zone\"]\n",
    "year = \"2025\"\n",
    "datacenters = config[\"ci\"][f\"{distance}\"][\"datacenters\"]\n",
    "locations = list(datacenters.keys())\n",
    "names = list(datacenters.values())\n",
    "\n",
    "flexibilities = snakemake.config[\"scenario\"][\"flexibility\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_nb(n, node, rename={}):\n",
    "    \"\"\"\n",
    "    Retrieve nodal energy balance per hour\n",
    "        -> lines and links are bidirectional AND their subsets are exclusive.\n",
    "        -> links include fossil gens\n",
    "    NB {-1} multiplier is a nodal balance sign\n",
    "    \"\"\"\n",
    "\n",
    "    components = [\"Generator\", \"Load\", \"StorageUnit\", \"Store\", \"Link\", \"Line\"]\n",
    "    nodal_balance = pd.DataFrame(index=n.snapshots)\n",
    "\n",
    "    for i in components:\n",
    "        if i == \"Generator\":\n",
    "            node_generators = n.generators.query(\"bus==@node\").index\n",
    "            nodal_balance = nodal_balance.join(n.generators_t.p[node_generators])\n",
    "        if i == \"Load\":\n",
    "            node_loads = n.loads.query(\"bus==@node\").index\n",
    "            nodal_balance = nodal_balance.join(-1 * n.loads_t.p_set[node_loads])\n",
    "        if i == \"Link\":\n",
    "            node_export_links = n.links.query(\"bus0==@node\").index\n",
    "            node_import_links = n.links.query(\"bus1==@node\").index\n",
    "            nodal_balance = nodal_balance.join(-1 * n.links_t.p0[node_export_links])\n",
    "            nodal_balance = nodal_balance.join(-1 * n.links_t.p1[node_import_links])\n",
    "            ##################\n",
    "        if i == \"StorageUnit\":\n",
    "            # node_storage_units = n.storage_units.query('bus==@node').index\n",
    "            # nodal_balance = nodal_balance.join(n.storage_units_t.p_dispatch[node_storage_units])\n",
    "            # nodal_balance = nodal_balance.join(n.storage_units_t.p_store[node_storage_units])\n",
    "            continue\n",
    "        if i == \"Line\":\n",
    "            continue\n",
    "        if i == \"Store\":\n",
    "            continue\n",
    "\n",
    "    nodal_balance = nodal_balance.rename(columns=rename).groupby(level=0, axis=1).sum()\n",
    "\n",
    "    # Custom groupby function\n",
    "    def custom_groupby(column_name):\n",
    "        if column_name.startswith(\"vcc\"):\n",
    "            return \"spatial shift\"\n",
    "        return column_name\n",
    "\n",
    "    # Apply custom groupby function\n",
    "    nodal_balance = nodal_balance.groupby(custom_groupby, axis=1).sum()\n",
    "\n",
    "    # revert nodal balance sign for display\n",
    "    if \"spatial shift\" in nodal_balance.columns:\n",
    "        nodal_balance[\"spatial shift\"] = nodal_balance[\"spatial shift\"] * -1\n",
    "    if \"temporal shift\" in nodal_balance.columns:\n",
    "        nodal_balance[\"temporal shift\"] = nodal_balance[\"temporal shift\"] * -1\n",
    "\n",
    "    return nodal_balance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_datacenter_shifts(n, dc1, dc2):\n",
    "    \"\"\"\n",
    "    Analyze the shifts in energy feed-in and spatial shift for two datacenters.\n",
    "\n",
    "    :param n: PyPSA Network object\n",
    "    :param dc1: Name of the first datacenter\n",
    "    :param dc2: Name of the second datacenter\n",
    "    :return: Dictionary with analysis results\n",
    "\n",
    "    NB Positive shift -> sending jobs away; negative shift -> receiving jobs\n",
    "    \"\"\"\n",
    "\n",
    "    # retrieve datacenter data\n",
    "    def analyze_dc(dc_name):\n",
    "        feedin = retrieve_nb(n, dc_name)[[f\"{dc_name} onwind\", f\"{dc_name} solar\"]]\n",
    "        curtailment = hourly_curtailment(n, \"onwind\", [dc_name]) + hourly_curtailment(\n",
    "            n, \"solar\", [dc_name]\n",
    "        )\n",
    "        spatial_shift = None\n",
    "        if \"spatial shift\" in retrieve_nb(n, dc_name):\n",
    "            spatial_shift = retrieve_nb(n, dc_name)[\"spatial shift\"]\n",
    "\n",
    "        return {\n",
    "            \"feedin\": feedin,\n",
    "            \"curtailment\": curtailment,\n",
    "            \"spatial_shift\": spatial_shift,\n",
    "        }\n",
    "\n",
    "    # Analyze both datacenters\n",
    "    dc1_analysis = analyze_dc(dc1)\n",
    "    dc2_analysis = analyze_dc(dc2)\n",
    "\n",
    "    # Collect and store wind and solar hourly potentials\n",
    "    potentials_dc1 = n.generators_t.p_max_pu[[f\"{dc1} onwind\", f\"{dc1} solar\"]]\n",
    "    potentials_dc2 = n.generators_t.p_max_pu[[f\"{dc2} onwind\", f\"{dc2} solar\"]]\n",
    "\n",
    "    # Compute differences between wind and solar feed-in\n",
    "    diff_onwind = (\n",
    "        dc1_analysis[\"feedin\"][f\"{dc1} onwind\"]\n",
    "        - dc2_analysis[\"feedin\"][f\"{dc2} onwind\"]\n",
    "    )\n",
    "    diff_solar = (\n",
    "        dc1_analysis[\"feedin\"][f\"{dc1} solar\"] - dc2_analysis[\"feedin\"][f\"{dc2} solar\"]\n",
    "    )\n",
    "\n",
    "    # Compute differences between wind and solar potentials\n",
    "    diff_onwind_potential = (\n",
    "        potentials_dc1[f\"{dc1} onwind\"] - potentials_dc2[f\"{dc2} onwind\"]\n",
    "    )\n",
    "    diff_solar_potential = (\n",
    "        potentials_dc1[f\"{dc1} solar\"] - potentials_dc2[f\"{dc2} solar\"]\n",
    "    )\n",
    "\n",
    "    return {\n",
    "        f\"{dc1}\": {\n",
    "            \"feedin\": dc1_analysis[\"feedin\"],\n",
    "            \"potentials\": potentials_dc1,\n",
    "            \"curtailment\": dc1_analysis[\"curtailment\"],\n",
    "            \"spatial_shift\": dc1_analysis[\"spatial_shift\"],\n",
    "        },\n",
    "        f\"{dc2}\": {\n",
    "            \"feedin\": dc2_analysis[\"feedin\"],\n",
    "            \"potentials\": potentials_dc2,\n",
    "            \"curtailment\": dc2_analysis[\"curtailment\"],\n",
    "            \"spatial_shift\": dc2_analysis[\"spatial_shift\"],\n",
    "        },\n",
    "        \"diff_generation\": {\"onwind\": diff_onwind, \"solar\": diff_solar},\n",
    "        \"diff_potentials\": {\n",
    "            \"onwind\": diff_onwind_potential,\n",
    "            \"solar\": diff_solar_potential,\n",
    "        },\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hourly_curtailment(network, tech, buses):\n",
    "    \"\"\"\n",
    "    Calculate the curtailment for a given technology and bus.\n",
    "    \"\"\"\n",
    "    weights = n.snapshot_weightings[\"generators\"]\n",
    "    gens = network.generators.query(\"carrier == @tech and bus in @buses\").index\n",
    "    curtailment = (\n",
    "        (\n",
    "            network.generators_t.p_max_pu[gens] * network.generators.p_nom_opt[gens]\n",
    "            - network.generators_t.p[gens]\n",
    "        )\n",
    "        .clip(lower=0)\n",
    "        .multiply(weights, axis=0)\n",
    "        .sum(axis=1)\n",
    "    )\n",
    "    return curtailment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_distance(n, bus1, bus2):\n",
    "    \"\"\"\n",
    "    Calculate the great circle distance between two buses in a PyPSA network object using the Haversine formula.\n",
    "\n",
    "    Parameters:\n",
    "    n (DataFrame): PyPSA network object containing bus coordinates.\n",
    "    bus1 (str): The ID of the first bus.\n",
    "    bus2 (str): The ID of the second bus.\n",
    "\n",
    "    Returns:\n",
    "    float: The distance between the two buses in kilometers.\n",
    "    \"\"\"\n",
    "\n",
    "    def haversine(lon1, lat1, lon2, lat2):\n",
    "        \"\"\"\n",
    "        Calculate the great circle distance between two points on the earth (specified in decimal degrees)\n",
    "        \"\"\"\n",
    "        # Convert decimal degrees to radians\n",
    "        lon1, lat1, lon2, lat2 = map(radians, [lon1, lat1, lon2, lat2])\n",
    "\n",
    "        dlon = lon2 - lon1\n",
    "        dlat = lat2 - lat1\n",
    "        a = sin(dlat / 2) ** 2 + cos(lat1) * cos(lat2) * sin(dlon / 2) ** 2\n",
    "        c = 2 * asin(sqrt(a))\n",
    "        r = 6371  # Radius of earth in kilometers.\n",
    "        return c * r\n",
    "\n",
    "    # Extract the coordinates of the two buses\n",
    "    lon1, lat1 = n.buses.loc[bus1, [\"x\", \"y\"]]\n",
    "    lon2, lat2 = n.buses.loc[bus2, [\"x\", \"y\"]]\n",
    "\n",
    "    # Calculate the distance using the Haversine formula\n",
    "    distance_km = haversine(lon1, lat1, lon2, lat2)\n",
    "\n",
    "    return distance_km"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dashboard plots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Costs vs Distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scenarios = [\n",
    "    \"IEIE\",\n",
    "    \"IENI\",\n",
    "    \"IEGB\",\n",
    "    \"IEDK\",\n",
    "    \"IENL\",\n",
    "]\n",
    "flexibilities = snakemake.config[\"scenario\"][\"flexibility\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "\n",
    "for scenario in scenarios:\n",
    "    for flexibility in flexibilities:\n",
    "        n = pypsa.Network(\n",
    "            f\"../{folder}/networks/2025/p1/cfe100/{scenario}/{flexibility}.nc\"\n",
    "        )\n",
    "\n",
    "        file_path = f\"..{folder}/summaries/2025/p1/cfe100/{scenario}/{flexibility}.yaml\"\n",
    "        with open(file_path, \"r\") as f:\n",
    "            summary = yaml.safe_load(f)\n",
    "\n",
    "            # if there is only one location\n",
    "            if len(summary) == 1:\n",
    "                location = next(iter(summary))\n",
    "                values = summary[location]\n",
    "                ci_average_cost = values.get(\"ci_average_cost\", None)\n",
    "                ci_total_cost = round(values.get(\"ci_total_cost\", None) / 1e6, 1)\n",
    "                if ci_average_cost is not None:\n",
    "                    data.append(\n",
    "                        (\n",
    "                            scenario,\n",
    "                            flexibility,\n",
    "                            location,\n",
    "                            0,  # Distance is 0 for a single location\n",
    "                            ci_average_cost,\n",
    "                            ci_total_cost,\n",
    "                        )\n",
    "                    )\n",
    "            else:\n",
    "                for location, values in summary.items():\n",
    "                    ci_average_cost = values.get(\"ci_average_cost\", None)\n",
    "                    ci_total_cost = round(values.get(\"ci_total_cost\", None) / 1e6, 1)\n",
    "                    if ci_average_cost is not None:\n",
    "                        for other_location in summary:\n",
    "                            if other_location != location:\n",
    "                                distance = round(\n",
    "                                    calculate_distance(n, location, other_location), 1\n",
    "                                )\n",
    "                                data.append(\n",
    "                                    (\n",
    "                                        scenario,\n",
    "                                        flexibility,\n",
    "                                        location,\n",
    "                                        distance,\n",
    "                                        ci_average_cost,\n",
    "                                        ci_total_cost,\n",
    "                                    )\n",
    "                                )\n",
    "\n",
    "df = pd.DataFrame(\n",
    "    data,\n",
    "    columns=[\n",
    "        \"Scenario\",\n",
    "        \"Flexibility\",\n",
    "        \"Location\",\n",
    "        \"Distance\",\n",
    "        \"CI_Average_Cost\",\n",
    "        \"CI_Total_Cost\",\n",
    "    ],\n",
    ")\n",
    "df.set_index([\"Scenario\", \"Flexibility\", \"Location\"], inplace=True)\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reset = df.reset_index()\n",
    "\n",
    "total_costs_df = (\n",
    "    df_reset.groupby([\"Scenario\", \"Flexibility\"])\n",
    "    .agg(\n",
    "        {\n",
    "            \"CI_Total_Cost\": \"sum\",\n",
    "            \"Distance\": \"mean\",\n",
    "        }\n",
    "    )\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "# Calculate Baseline Costs (0% Flexibility) for each Scenario\n",
    "baseline_costs = total_costs_df[total_costs_df[\"Flexibility\"] == \"0\"][\n",
    "    [\"Scenario\", \"CI_Total_Cost\"]\n",
    "].rename(columns={\"CI_Total_Cost\": \"Baseline_Cost\"})\n",
    "\n",
    "df_with_baseline = total_costs_df.merge(baseline_costs, on=\"Scenario\")\n",
    "\n",
    "# Calculate Cost Savings for each Scenario and Flexibility compared to Baseline\n",
    "df_with_baseline[\"Cost_Savings\"] = round(\n",
    "    100\n",
    "    * (df_with_baseline[\"Baseline_Cost\"] - df_with_baseline[\"CI_Total_Cost\"])\n",
    "    / df_with_baseline[\"Baseline_Cost\"],\n",
    "    2,\n",
    ")\n",
    "\n",
    "df_with_baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmap = plt.get_cmap(\"magma\")\n",
    "colors = [cmap(i) for i in np.linspace(0, 1, 7)]\n",
    "cmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_cost_savings(ax, df):\n",
    "    \"\"\"\n",
    "    Plot cost savings as a function of distance with different hues for each flexibility scenario using Seaborn.\n",
    "\n",
    "    Parameters:\n",
    "    df (pandas.DataFrame): DataFrame containing 'Distance', 'Cost_Savings', and 'Flexibility' columns.\n",
    "    \"\"\"\n",
    "\n",
    "    # Set the style\n",
    "    sns.set(style=\"ticks\")\n",
    "    sns.set_context(rc={\"lines.linewidth\": 4})\n",
    "\n",
    "    # fig = plt.figure(figsize=(8, 5), dpi=100)\n",
    "\n",
    "    # Generate a custom color palette from 'viridis'\n",
    "    cmap = plt.get_cmap(\"magma\")\n",
    "    colors = cmap(np.linspace(0, 1, df[\"Flexibility\"].nunique() + 1))\n",
    "\n",
    "    # Create a line plot with the custom color palette\n",
    "    lineplot = sns.lineplot(\n",
    "        x=\"Distance\",\n",
    "        y=\"Cost_Savings\",\n",
    "        hue=\"Flexibility\",\n",
    "        style=\"Flexibility\",\n",
    "        data=df,\n",
    "        markers=True,  # changed from \"o\" to True for compatibility\n",
    "        dashes=False,\n",
    "        palette=colors,  # Use the custom colors\n",
    "    )\n",
    "\n",
    "    # Set title and labels with specified font sizes\n",
    "    plt.title(\n",
    "        \"Cost savings with datacenter distance and share of flexible loads\", fontsize=14\n",
    "    )\n",
    "    plt.xlabel(\"Haversine distance between datacenter pair (km)\", fontsize=14)\n",
    "    plt.ylabel(\"Cost Savings relative to baseline (%)\", fontsize=14)\n",
    "\n",
    "    # Adjust the legend to show only line markers\n",
    "    handles, labels = lineplot.get_legend_handles_labels()\n",
    "    modified_labels = [\n",
    "        label + \"%\" if not label.startswith(\"Flexibility\") else label\n",
    "        for label in labels\n",
    "    ]\n",
    "\n",
    "    # Modify labels if needed, keeping your customization\n",
    "    plt.legend(\n",
    "        loc=\"center left\",\n",
    "        fontsize=\"small\",\n",
    "        ncol=1,\n",
    "        handles=handles,\n",
    "        labels=modified_labels,\n",
    "        prop={\"size\": 12},\n",
    "    )\n",
    "\n",
    "    # udjust Y-axis limits\n",
    "    y_min, y_max = plt.gca().get_ylim()\n",
    "    y_range = y_max - y_min\n",
    "    new_y_max = y_max + y_range * 0.1\n",
    "    plt.gca().set_ylim(y_min, new_y_max)\n",
    "\n",
    "    # Filter DataFrame for unique Distance-Scenario pairs for one flexibility level\n",
    "    unique_distances_scenarios = df.drop_duplicates(subset=[\"Distance\", \"Scenario\"])\n",
    "\n",
    "    for _, row in unique_distances_scenarios.iterrows():\n",
    "        plt.axvline(x=row[\"Distance\"], color=\"grey\", linestyle=\"--\", linewidth=0.5)\n",
    "        label_y_position = (\n",
    "            plt.gca().get_ylim()[1]\n",
    "            - (plt.gca().get_ylim()[1] - plt.gca().get_ylim()[0]) * 0.01\n",
    "        )  # 5% down from the top\n",
    "        plt.text(\n",
    "            row[\"Distance\"],\n",
    "            label_y_position,\n",
    "            f\"{row['Scenario']}\",\n",
    "            rotation=0,\n",
    "            ha=\"right\",\n",
    "            va=\"top\",\n",
    "            fontsize=14,\n",
    "        )\n",
    "\n",
    "    # make background of the plot transparent\n",
    "    plt.gca().patch.set_alpha(0)\n",
    "\n",
    "    # #    plt.show()\n",
    "    # fig.tight_layout()\n",
    "    # fig.savefig(\n",
    "    #     \"../manuscript/img/distance-costs.pdf\",\n",
    "    #     transparent=True,\n",
    "    # )\n",
    "\n",
    "\n",
    "# plot_cost_savings(df_with_baseline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ### Cost savings VS wind feed-in correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ci_config = snakemake.config[\"ci\"]\n",
    "\n",
    "\n",
    "def get_datacenter_name(scenario, location_code):\n",
    "    # Retrieve the datacenter mapping for the given scenario\n",
    "    datacenter_mapping = ci_config[scenario][\"datacenters\"]\n",
    "    return datacenter_mapping.get(location_code)\n",
    "\n",
    "\n",
    "# Function to retrieve the base node for a scenario\n",
    "def get_base_node(scenario):\n",
    "    datacenters = ci_config[scenario][\"datacenters\"]\n",
    "    return next(iter(datacenters))  # Always return the first datacenter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset the index of 'df' to work with columns directly\n",
    "df_reset = df.reset_index()\n",
    "\n",
    "# Filter out the entries for 0% Flexibility\n",
    "df_flex_zero = df_reset[df_reset[\"Flexibility\"] == \"0\"]\n",
    "\n",
    "# Group by 'Scenario' and sum up the 'CI_Total_Cost' for each scenario\n",
    "# This gives the baseline cost for each scenario at 0% Flexibility\n",
    "baseline_costs = (\n",
    "    df_flex_zero.groupby(\"Scenario\")\n",
    "    .agg({\"CI_Total_Cost\": \"sum\"})\n",
    "    .rename(columns={\"CI_Total_Cost\": \"Baseline_Cost\"})\n",
    ")\n",
    "baseline_costs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enhanced_data = []\n",
    "all_locations = df.index.get_level_values(\"Location\").unique()\n",
    "\n",
    "\n",
    "for (scenario, flexibility), group in df.groupby([\"Scenario\", \"Flexibility\"]):\n",
    "    n = pypsa.Network(\n",
    "        f\"../{folder}/networks/2025/p1/cfe100/{scenario}/{flexibility}.nc\"\n",
    "    )\n",
    "    # Fetch the baseline cost for the scenario\n",
    "    baseline_cost = (\n",
    "        baseline_costs.loc[scenario, \"Baseline_Cost\"]\n",
    "        if scenario in baseline_costs.index\n",
    "        else np.nan\n",
    "    )\n",
    "\n",
    "    base_node = get_base_node(scenario)\n",
    "\n",
    "    datacenter_name_1 = get_datacenter_name(scenario, base_node)\n",
    "    if datacenter_name_1 is None:\n",
    "        continue\n",
    "\n",
    "    wind_series_1 = analyze_datacenter_shifts(\n",
    "        n, dc1=datacenter_name_1, dc2=datacenter_name_1\n",
    "    )[datacenter_name_1][\"potentials\"][f\"{datacenter_name_1} onwind\"]\n",
    "\n",
    "    for loc2 in all_locations:\n",
    "        if base_node != loc2:\n",
    "            datacenter_name_2 = get_datacenter_name(scenario, loc2)\n",
    "            if datacenter_name_2 is None:\n",
    "                continue\n",
    "\n",
    "            wind_series_2 = analyze_datacenter_shifts(\n",
    "                n, dc1=datacenter_name_2, dc2=datacenter_name_2\n",
    "            )[datacenter_name_2][\"potentials\"][f\"{datacenter_name_2} onwind\"]\n",
    "            correlation = np.corrcoef(wind_series_1, wind_series_2)[0, 1]\n",
    "\n",
    "            mean_distance = group.xs(base_node, level=\"Location\")[\"Distance\"].mean()\n",
    "            total_cost = group[\"CI_Total_Cost\"].sum()\n",
    "\n",
    "            # Corrected cost savings calculation\n",
    "            if pd.notna(baseline_cost) and pd.notna(total_cost):\n",
    "                cost_savings = 100 * (baseline_cost - total_cost) / baseline_cost\n",
    "                cost_savings = round(cost_savings, 2)\n",
    "            else:\n",
    "                cost_savings = np.nan\n",
    "\n",
    "            enhanced_data.append(\n",
    "                (\n",
    "                    scenario,\n",
    "                    flexibility,\n",
    "                    base_node,\n",
    "                    loc2,\n",
    "                    mean_distance,\n",
    "                    total_cost,\n",
    "                    correlation,\n",
    "                    cost_savings,\n",
    "                    baseline_cost,\n",
    "                )\n",
    "            )\n",
    "\n",
    "df_enhanced = pd.DataFrame(\n",
    "    enhanced_data,\n",
    "    columns=[\n",
    "        \"Scenario\",\n",
    "        \"Flexibility\",\n",
    "        \"Location\",\n",
    "        \"Other_Location\",\n",
    "        \"Mean_Distance\",\n",
    "        \"Total_Cost\",\n",
    "        \"Wind_Correlation\",\n",
    "        \"Cost_Savings\",\n",
    "        \"Baseline_Cost\",\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_enhanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_cost_savings_correlation(ax, df):\n",
    "    \"\"\"\n",
    "    Plot cost savings as a function of wind correlation for each flexibility scenario.\n",
    "\n",
    "    Parameters:\n",
    "    df (pandas.DataFrame): DataFrame containing 'Wind_Correlation', 'Cost_Savings', and 'Flexibility' columns.\n",
    "    \"\"\"\n",
    "\n",
    "    sns.set(style=\"ticks\")\n",
    "    sns.set_context(rc={\"lines.linewidth\": 4})\n",
    "\n",
    "    cmap = plt.get_cmap(\"magma\")\n",
    "    colors = cmap(np.linspace(0, 1, df[\"Flexibility\"].nunique() + 1))\n",
    "\n",
    "    # plt.figure(figsize=(8, 5), dpi=100)\n",
    "\n",
    "    lineplot = sns.lineplot(\n",
    "        x=\"Wind_Correlation\",\n",
    "        y=\"Cost_Savings\",\n",
    "        hue=\"Flexibility\",\n",
    "        style=\"Flexibility\",\n",
    "        data=df,\n",
    "        markers=True,  # changed from \"o\" to True for compatibility\n",
    "        dashes=False,\n",
    "        palette=colors,\n",
    "    )\n",
    "\n",
    "    plt.title(\n",
    "        \"Impact of wind correlation and flexible load share on cost savings\",\n",
    "        fontsize=14,\n",
    "    )\n",
    "    plt.xlabel(\n",
    "        \"Wind correlation (Pearson's r) between datacenter locations\", fontsize=14\n",
    "    )\n",
    "    plt.ylabel(\"Cost savings relative to baseline (%)\", fontsize=14)\n",
    "\n",
    "    handles, labels = lineplot.get_legend_handles_labels()\n",
    "    modified_labels = [\n",
    "        label + \"%\" if not label.startswith(\"Flexibility\") else label\n",
    "        for label in labels\n",
    "    ]\n",
    "    plt.legend(\n",
    "        loc=\"center right\",\n",
    "        fontsize=\"small\",\n",
    "        ncol=1,\n",
    "        handles=handles,\n",
    "        labels=modified_labels,\n",
    "        prop={\"size\": 12},\n",
    "    )\n",
    "\n",
    "    # Adjust Y-axis limits\n",
    "    y_min, y_max = plt.gca().get_ylim()\n",
    "    y_range = y_max - y_min\n",
    "    new_y_max = y_max + y_range * 0.1\n",
    "    plt.gca().set_ylim(y_min, new_y_max)\n",
    "\n",
    "    unique_correlations = df[df[\"Flexibility\"] == \"0\"].drop_duplicates(\n",
    "        subset=[\"Wind_Correlation\"]\n",
    "    )\n",
    "\n",
    "    for _, row in unique_correlations.iterrows():\n",
    "        plt.axvline(\n",
    "            x=row[\"Wind_Correlation\"], color=\"grey\", linestyle=\"--\", linewidth=0.5\n",
    "        )\n",
    "        label_y_position = y_max + y_range * 0.09\n",
    "        plt.text(\n",
    "            row[\"Wind_Correlation\"],\n",
    "            label_y_position,\n",
    "            f\"{row['Scenario']}\",\n",
    "            rotation=0,\n",
    "            ha=\"left\",\n",
    "            va=\"top\",\n",
    "            fontsize=14,\n",
    "        )\n",
    "\n",
    "    # plt.tight_layout()\n",
    "    # plt.savefig(\"../manuscript/img/wind_correlation-costs.pdf\", transparent=True)\n",
    "\n",
    "    # plt.show()\n",
    "\n",
    "\n",
    "# plot_cost_savings_correlation(df)  # Uncomment and replace 'df' with your actual DataFrame variable name to use the function\n",
    "# plot_cost_savings_correlation(df_enhanced)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regions = gpd.read_file(\"../input/regions_onshore_elec_s_256.geojson\")\n",
    "n = pypsa.Network(\"../input/elec_s_256_ec.nc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_pearson_correlation_falloff(network, carrier, base_region):\n",
    "    \"\"\"\n",
    "    Computes the Pearson correlation coefficient between the p_max_pu time series\n",
    "    of a specified base region for a given carrier and all other regions.\n",
    "\n",
    "    Returns:\n",
    "    - A DataFrame with each region and its correlation coefficient with the base region.\n",
    "    \"\"\"\n",
    "\n",
    "    feedin = network.generators_t.p_max_pu.filter(like=carrier)\n",
    "\n",
    "    base_series = feedin[f\"{base_region} {carrier}\"]\n",
    "\n",
    "    correlations = []\n",
    "    for region in feedin.columns:\n",
    "        other_series = feedin[region]\n",
    "        correlation = base_series.corr(other_series)\n",
    "\n",
    "        region_name = \" \".join(region.split()[:-1])\n",
    "        correlations.append((region_name, correlation))\n",
    "\n",
    "    correlation_df = pd.DataFrame(correlations, columns=[\"Region\", \"Correlation\"])\n",
    "    return correlation_df.sort_values(\"Correlation\", ascending=False)\n",
    "\n",
    "\n",
    "correlation_df = compute_pearson_correlation_falloff(n, \"onwind\", \"DK1 0\")\n",
    "print(correlation_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_pearson_falloff(\n",
    "    ax, regions, network, carrier=\"onwind\", base_region=\"DK1 0\", colormap=\"viridis\"\n",
    "):\n",
    "    data = compute_pearson_correlation_falloff(n, carrier, base_region)\n",
    "\n",
    "    merged_data = regions.merge(data, left_on=\"name\", right_on=\"Region\", how=\"left\")\n",
    "\n",
    "    # fig, ax = plt.subplots(\n",
    "    #     subplot_kw={\"projection\": ccrs.PlateCarree()}, figsize=(10, 8)\n",
    "    # )\n",
    "\n",
    "    ax.set_extent([-15, 30, 35, 60], crs=ccrs.PlateCarree())\n",
    "\n",
    "    map_opts = {\n",
    "        \"color_geomap\": {\n",
    "            \"ocean\": \"lightblue\",\n",
    "            \"land\": \"white\",\n",
    "            \"border\": \"black\",\n",
    "            \"coastline\": \"black\",\n",
    "        },\n",
    "    }\n",
    "\n",
    "    # Add map features with custom styles\n",
    "    ax.add_feature(\n",
    "        cfeature.OCEAN.with_scale(\"50m\"),\n",
    "        facecolor=map_opts[\"color_geomap\"][\"ocean\"],\n",
    "        zorder=0,\n",
    "    )\n",
    "    ax.add_feature(\n",
    "        cfeature.LAND.with_scale(\"50m\"),\n",
    "        facecolor=map_opts[\"color_geomap\"][\"land\"],\n",
    "        zorder=0,\n",
    "    )\n",
    "    ax.add_feature(\n",
    "        cfeature.BORDERS.with_scale(\"50m\"),\n",
    "        edgecolor=map_opts[\"color_geomap\"][\"border\"],\n",
    "        linewidth=0.02,\n",
    "        alpha=0.3,\n",
    "        zorder=1,\n",
    "        rasterized=True,\n",
    "    )\n",
    "    ax.add_feature(\n",
    "        cfeature.COASTLINE.with_scale(\"50m\"),\n",
    "        edgecolor=map_opts[\"color_geomap\"][\"coastline\"],\n",
    "        linewidth=0.02,\n",
    "        alpha=0.3,\n",
    "        zorder=1,\n",
    "        rasterized=True,\n",
    "    )\n",
    "\n",
    "    merged_data.plot(\n",
    "        column=\"Correlation\",\n",
    "        ax=ax,\n",
    "        legend=False,\n",
    "        cmap=colormap,\n",
    "        linewidth=0.1,\n",
    "        vmin=0,\n",
    "        vmax=1,\n",
    "    )\n",
    "\n",
    "    norm = mpl.colors.Normalize(\n",
    "        vmin=merged_data[\"Correlation\"].min(),\n",
    "        vmax=merged_data[\"Correlation\"].max(),\n",
    "    )\n",
    "    sm = mpl.cm.ScalarMappable(cmap=colormap, norm=norm)\n",
    "    sm.set_array([])  # You need this line for the colorbar to work with ScalarMappable.\n",
    "\n",
    "    # Create colorbar\n",
    "    cbar = ax.figure.colorbar(\n",
    "        sm, ax=ax, orientation=\"horizontal\", fraction=0.05, pad=0.01\n",
    "    )\n",
    "\n",
    "    if carrier == \"onwind\":\n",
    "        _ = \"onshore wind\"\n",
    "    if carrier == \"solar\":\n",
    "        _ = \"solar PV\"\n",
    "\n",
    "    ax.coastlines()\n",
    "    ax.set_title(\n",
    "        f\"Wind correlation (Pearson's r) falloff with distance \\n\"\n",
    "        f\"data: {_} hourly capacity factor; base region: {base_region.split()[0]}\",\n",
    "        fontsize=14,\n",
    "    )\n",
    "\n",
    "    # plt.show()\n",
    "    # plt.savefig(\n",
    "    #     f\"../manuscript/img/pearson_corr_falloff_{carrier}_{base_region}.pdf\",\n",
    "    #     bbox_inches=\"tight\",\n",
    "    #     transparent=True,\n",
    "    # )\n",
    "\n",
    "\n",
    "# plot_pearson_falloff(\n",
    "#     regions, n, carrier=\"onwind\", base_region=\"DK1 0\", colormap=\"magma\"\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### heatmaps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_heatmap_shifts(n, shift_type, location, scaling):\n",
    "    year = 2013\n",
    "    data = (\n",
    "        retrieve_nb(n=n, node=f\"{location}\").get(f\"{shift_type}\") * -1\n",
    "    )  # NB REVERTING SIGN\n",
    "\n",
    "    days = np.arange(1, 366)\n",
    "    num_days = 365  # Correct number of days for a non-leap year\n",
    "    values = np.empty((int(24 / scaling), num_days))\n",
    "    values[:] = np.NaN  # Fill with NaNs as a default value\n",
    "\n",
    "    for month in range(1, 13):\n",
    "        month_data = data[data.index.month == month]\n",
    "        for day in month_data.index.day.unique():\n",
    "            day_idx = (\n",
    "                sum([calendar.monthrange(year, m)[1] for m in range(1, month)])\n",
    "                + day\n",
    "                - 1\n",
    "            )  # Adjusting day_idx to be zero-indexed\n",
    "            day_values = month_data[month_data.index.day == day].values\n",
    "            try:\n",
    "                # Ensuring day_values is a 1D array before assignment\n",
    "                values[:, day_idx] = day_values.squeeze()\n",
    "            except ValueError as e:\n",
    "                print(f\"Error processing day {day} of month {month}: {e}\")\n",
    "\n",
    "    return days, values\n",
    "\n",
    "\n",
    "def draw_heatmap(ax, day, value, scaling, colormap, min_val, max_val):\n",
    "    xgrid = np.arange(day.max() + 1) + 1  # for days\n",
    "    ygrid = np.arange(int(24 / scaling) + 1)  # for hours\n",
    "\n",
    "    # Ensure the dimensions of 'value' match the expected dimensions for 'xgrid' and 'ygrid'\n",
    "    if value.shape != (len(ygrid) - 1, len(xgrid) - 1):\n",
    "        raise ValueError(\n",
    "            f\"Shape of value ({value.shape}) does not match xgrid ({len(xgrid)}) and ygrid ({len(ygrid)}) dimensions.\"\n",
    "        )\n",
    "\n",
    "    ax.pcolormesh(xgrid, ygrid, value, cmap=colormap, vmin=min_val, vmax=max_val)\n",
    "    ax.set_ylim(int(24 / scaling), 0)\n",
    "\n",
    "    # Y-axis: display specific hours\n",
    "    hour_ticks = np.arange(0, 25, 6) / scaling  # Adjusted for scaling\n",
    "    ax.set_yticks(hour_ticks)\n",
    "    ax.set_yticklabels([\"0:00\", \"6:00\", \"12:00\", \"18:00\", \"24:00\"], fontsize=14)\n",
    "\n",
    "    # X-axis: display month names\n",
    "    month_positions = (\n",
    "        np.cumsum([0] + [calendar.monthrange(2013, i)[1] for i in range(1, 12)]) + 15\n",
    "    )  # alright, let the tick be at the middle of each month\n",
    "    month_labels = [calendar.month_abbr[i] for i in range(1, 13)]\n",
    "    ax.set_xticks(month_positions)\n",
    "    ax.set_xticklabels(month_labels, fontsize=14)\n",
    "\n",
    "    ax.axis(\"on\")\n",
    "\n",
    "\n",
    "# def add_custom_annotations(ax):\n",
    "#     ax.text(\n",
    "#         -0.005,\n",
    "#         0.25,\n",
    "#         \"Hour of the Day\",\n",
    "#         transform=ax.transAxes,\n",
    "#         rotation=\"vertical\",\n",
    "#         ha=\"right\",\n",
    "#         va=\"center\",\n",
    "#         fontsize=14,\n",
    "#     )\n",
    "#     ax.text(\n",
    "#         0.1,\n",
    "#         -0.01,\n",
    "#         \"Day of the Year\",\n",
    "#         transform=ax.transAxes,\n",
    "#         ha=\"center\",\n",
    "#         va=\"top\",\n",
    "#         fontsize=14,\n",
    "#     )\n",
    "\n",
    "\n",
    "def plot_heatmap_shifts(\n",
    "    fig,\n",
    "    ax,\n",
    "    shift_type,\n",
    "    location,\n",
    "    scaling,\n",
    "    colormap,\n",
    "    min_val,\n",
    "    max_val,\n",
    "):\n",
    "    # fig, axes = plt.subplots(1, 12, figsize=figsize, sharey=True)\n",
    "    # plt.tight_layout()\n",
    "\n",
    "    # add_custom_annotations(ax)\n",
    "\n",
    "    selected_scen = pypsa.Network(f\"../{folder}/networks/2025/p1/cfe100/IEDK/40.nc\")\n",
    "\n",
    "    days, value = prepare_heatmap_shifts(selected_scen, shift_type, location, scaling)\n",
    "\n",
    "    draw_heatmap(ax, days, value, scaling, colormap, min_val, max_val)\n",
    "\n",
    "    ax.set_title(\n",
    "        f\"{location} (vs Ireland pair) - {shift_type} \" + r\"[MWhÂ·h$^{-1}$]\",\n",
    "        fontsize=14,\n",
    "        pad=3,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### datacenter map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Plot 1\n",
    "\n",
    "\n",
    "def assign_location(n):\n",
    "    \"\"\"\n",
    "    Assign bus location per each individual component\n",
    "    \"\"\"\n",
    "    for c in n.iterate_components(n.one_port_components | n.branch_components):\n",
    "        ifind = pd.Series(c.df.index.str.find(\" \", start=4), c.df.index)\n",
    "        for i in ifind.value_counts().index:\n",
    "            # these have already been assigned defaults\n",
    "            if i == -1:\n",
    "                continue\n",
    "            names = ifind.index[ifind == i]\n",
    "            c.df.loc[names, \"location\"] = names.str[:i]\n",
    "\n",
    "\n",
    "def plot_datacenters_on_europe_map(ax, network, datacenters):\n",
    "    # projection = ccrs.PlateCarree()\n",
    "    # fig, ax = plt.subplots(figsize=(10, 8), subplot_kw={\"projection\": projection})\n",
    "\n",
    "    ax.set_extent([-15, 30, 30, 70], crs=ccrs.PlateCarree())\n",
    "\n",
    "    map_opts = {\n",
    "        \"color_geomap\": {\n",
    "            \"ocean\": \"lightblue\",\n",
    "            \"land\": \"white\",\n",
    "            \"border\": \"black\",\n",
    "            \"coastline\": \"black\",\n",
    "        },\n",
    "    }\n",
    "\n",
    "    # Add map features with custom styles\n",
    "    ax.add_feature(\n",
    "        cfeature.OCEAN.with_scale(\"50m\"),\n",
    "        facecolor=map_opts[\"color_geomap\"][\"ocean\"],\n",
    "        zorder=0,\n",
    "    )\n",
    "    ax.add_feature(\n",
    "        cfeature.LAND.with_scale(\"50m\"),\n",
    "        facecolor=map_opts[\"color_geomap\"][\"land\"],\n",
    "        zorder=0,\n",
    "    )\n",
    "    ax.add_feature(\n",
    "        cfeature.BORDERS.with_scale(\"50m\"),\n",
    "        edgecolor=map_opts[\"color_geomap\"][\"border\"],\n",
    "        linewidth=0.5,\n",
    "        alpha=0.5,\n",
    "        zorder=1,\n",
    "    )\n",
    "    ax.add_feature(\n",
    "        cfeature.COASTLINE.with_scale(\"50m\"),\n",
    "        edgecolor=map_opts[\"color_geomap\"][\"coastline\"],\n",
    "        linewidth=0.5,\n",
    "        alpha=0.5,\n",
    "        zorder=1,\n",
    "    )\n",
    "\n",
    "    n = network.copy()\n",
    "    assign_location(n)\n",
    "\n",
    "    n.buses.drop(n.buses.index[n.buses.carrier != \"AC\"], inplace=True)\n",
    "    n.stores.drop(n.stores[n.stores.index.str.contains(\"EU\")].index, inplace=True)\n",
    "    n.links.drop(n.links[n.links.carrier == \"dsm\"].index, inplace=True)\n",
    "\n",
    "    dc_locations = n.buses.loc[datacenters.keys(), [\"x\", \"y\"]]\n",
    "\n",
    "    ax.scatter(\n",
    "        dc_locations[\"x\"],\n",
    "        dc_locations[\"y\"],\n",
    "        color=\"darkblue\",\n",
    "        # label=\"Datacenter locations\",\n",
    "        transform=ccrs.Geodetic(),\n",
    "        zorder=5,\n",
    "        s=100,\n",
    "    )\n",
    "    ax.set_title(\"Datacenter locations\", fontsize=14)\n",
    "\n",
    "    # Cherry-pick Ireland's location\n",
    "    ireland_location = dc_locations.loc[\"IE5 0\"]\n",
    "\n",
    "    # Plot lines from Ireland to each other datacenter\n",
    "    for location_id, location in dc_locations.iterrows():\n",
    "        if location_id != \"IE5 0\":  # Exclude line from Ireland to itself\n",
    "            ax.plot(\n",
    "                [ireland_location[\"x\"], location[\"x\"]],\n",
    "                [ireland_location[\"y\"], location[\"y\"]],\n",
    "                color=\"darkblue\",\n",
    "                linestyle=\"dotted\",\n",
    "                alpha=0.8,\n",
    "                transform=ccrs.Geodetic(),\n",
    "                zorder=4,\n",
    "                linewidth=2,\n",
    "            )\n",
    "\n",
    "    # Customize the plot\n",
    "    plt.legend(loc=\"lower left\")\n",
    "\n",
    "    # plt.show()\n",
    "\n",
    "    # fig.tight_layout()\n",
    "    # fig.savefig(\n",
    "    #     snakemake.output.plot_DC,\n",
    "    #     facecolor=\"white\",\n",
    "    #     dpi=600,\n",
    "    # )\n",
    "\n",
    "\n",
    "# # this definition is manual for brevity\n",
    "datacenters = {\n",
    "    \"IE5 0\": \"Ireland\",\n",
    "    \"GB5 0\": \"Northern Ireland\",\n",
    "    \"GB0 0\": \"Great Britain\",\n",
    "    \"NL1 0\": \"Netherlands\",\n",
    "    \"DK1 0\": \"Denmark\",\n",
    "}\n",
    "\n",
    "# plot_datacenters_on_europe_map(network=selected_scen, datacenters=datacenters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dashboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dashboard(\n",
    "    df_with_baseline,\n",
    "    network,\n",
    "    # datacenters,\n",
    "    regions,\n",
    "    colormap,\n",
    "):\n",
    "    \"\"\"\n",
    "    Creates a dashboard for section 2: wind correlation\n",
    "    \"\"\"\n",
    "    # Create figure\n",
    "    # plt.figure(figsize=(30, 50))\n",
    "\n",
    "    fig = plt.figure(figsize=(30, 30))\n",
    "\n",
    "    # Define GridSpec: 5 rows, 6 columns\n",
    "    gs = gridspec.GridSpec(42, 6, hspace=2, wspace=0.3)\n",
    "\n",
    "    ax_map = plt.subplot(gs[16:30, :2], projection=ccrs.PlateCarree())\n",
    "    plot_datacenters_on_europe_map(\n",
    "        ax_map, network, datacenters\n",
    "    )  # Custom function for the map\n",
    "\n",
    "    ax_map_2 = plt.subplot(gs[0:30, 2:4], projection=ccrs.PlateCarree())\n",
    "    plot_pearson_falloff(\n",
    "        ax=ax_map_2,\n",
    "        regions=regions,\n",
    "        network=network,\n",
    "        carrier=\"onwind\",\n",
    "        base_region=\"DK1 0\",\n",
    "        colormap=\"magma\",\n",
    "    )\n",
    "\n",
    "    ax_map_3 = plt.subplot(gs[0:30, 4:6], projection=ccrs.PlateCarree())\n",
    "    plot_pearson_falloff(\n",
    "        ax=ax_map_3,\n",
    "        regions=regions,\n",
    "        network=network,\n",
    "        carrier=\"onwind\",\n",
    "        base_region=\"IE5 0\",\n",
    "        colormap=\"magma\",\n",
    "    )\n",
    "\n",
    "    # Key plots\n",
    "    ax_line_1 = plt.subplot(gs[30:40, 3:6])\n",
    "    plot_cost_savings(ax=ax_line_1, df=df_with_baseline)\n",
    "\n",
    "    # ax_line_2 = plt.subplot(gs[30:40, 3:6])\n",
    "    # plot_cost_savings_correlation(ax=ax_line_2, df=df_enhanced)\n",
    "\n",
    "    # heatmap spatial shifts plot\n",
    "    heatmap_ax_1 = plt.subplot(gs[30:38, 0:3])\n",
    "    plot_heatmap_shifts(\n",
    "        fig=fig,\n",
    "        ax=heatmap_ax_1,\n",
    "        shift_type=\"spatial shift\",\n",
    "        location=\"Denmark\",\n",
    "        scaling=1,\n",
    "        colormap=colormap,\n",
    "        min_val=-40.0,  # - int(flex)\n",
    "        max_val=+40.0,  # + int(flex)\n",
    "    )\n",
    "\n",
    "    # Creating a colorbar specifically for the heatmap subplot\n",
    "\n",
    "    cbar_ax = fig.add_axes(\n",
    "        [0.135, 0.16, 0.36, 0.008]\n",
    "    )  # These values may need adjustment\n",
    "    norm = mc.Normalize(vmin=-40.0, vmax=+40.0)\n",
    "    cb = fig.colorbar(\n",
    "        ScalarMappable(norm=norm, cmap=colormap),\n",
    "        cax=cbar_ax,\n",
    "        orientation=\"horizontal\",\n",
    "    )\n",
    "    cb.ax.tick_params(labelsize=14)\n",
    "\n",
    "    fig.text(\n",
    "        0.135,\n",
    "        0.133,\n",
    "        \"Positive values mapped to blue color represent increase of a load\",\n",
    "        ha=\"left\",\n",
    "        fontsize=14,\n",
    "    )\n",
    "\n",
    "    fig.text(\n",
    "        0.135,\n",
    "        0.14,\n",
    "        \"Negative values mapped to red color represent decrease of a load\",\n",
    "        ha=\"left\",\n",
    "        fontsize=14,\n",
    "    )\n",
    "\n",
    "    fig.text(0.115, 0.57, \"a\", ha=\"left\", fontsize=20, fontweight=\"bold\")\n",
    "    fig.text(0.38, 0.59, \"b\", ha=\"left\", fontsize=20, fontweight=\"bold\")\n",
    "    fig.text(0.65, 0.59, \"c\", ha=\"left\", fontsize=20, fontweight=\"bold\")\n",
    "    fig.text(0.115, 0.325, \"d\", ha=\"left\", fontsize=20, fontweight=\"bold\")\n",
    "    fig.text(0.515, 0.325, \"e\", ha=\"left\", fontsize=20, fontweight=\"bold\")\n",
    "\n",
    "    # plt.show()\n",
    "    # fig.tight_layout()\n",
    "    fig.savefig(\n",
    "        \"../manuscript/img/dashboard_2.png\",\n",
    "        dpi=300,\n",
    "        bbox_inches=\"tight\",\n",
    "    )\n",
    "\n",
    "\n",
    "create_dashboard(\n",
    "    df_with_baseline=df_with_baseline,\n",
    "    network=n,\n",
    "    #  datacenters=datacenters,\n",
    "    regions=regions,\n",
    "    colormap=\"RdBu\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "",
   "language": "python",
   "name": ""
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
